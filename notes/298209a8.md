---
author: jaimecgomezz
title: Full text search
date: 05-12-2024
tags: [postgresql]
---

# Full text search

Full text search provides the capability to identify natural-language `documents` that satisfy a `query`, and optionally to sort them by relevance to the query. Along with the searching capabilities, indexing allows documents to be preprocessed and an index saved for later rapid searching.

## Preprocessing

1. Parsing documents into tokens

    The document is decomposed into various classes of tokens, e.g. numbers, words, email addresses, so they can be processed differently. PostgreSQL uses a `parser` to perform this step.

2. Converting tokens into lexeme

    A lexeme is a string, just like a token, but it has been `normalized` so that different forms of the same word are made alike. This allows searches to find variant forms of the same word without tediously specifying each one. In short, tokens are raw fragments of the document text, while lexemes are words that are believed useful for searching and indexing. PostgreSQL uses `dictionaries` for this step.

3. Storing preprocessed documents optimized for searching

    Each document can be represented as a sorted array of normalized lexemes. Along with the lexemes it is often desirable to store positional information to use for `proximity ranking`, so a document with more *dense* region of query words is assigned a higher rank that one with scattered query words.

## Documents
A `document` is the unit of searching in a full text search system. The text search system must be able to parse documents and store associations of lexemes with their parent document.

For text search purposes, each document must be reduced to the preprocessed `tsvector` format. Searching and ranking are performed entirely on the `tsvector` representation of a document -- the original text need only be retrieved when the document has been selected for display. In short, a `tsvector` is the compact representation of a `document`.


## Text matching
Full text searching in PostgreSQL is based on the `match` operator `@@`, which returns `true` if a `tsvector` matches a `tsquery`. It doesn't matter the order on which they are specified.

```sql
SELECT 'a fat cat sat on a mat and ate a fat rat'::tsvector @@ 'cat & rat'::tsquery;
--  ?column?
-- ----------
--  t
```

```sql
SELECT 'fat & cow'::tsquery @@ 'a fat cat sat on a mat and ate a fat rat'::tsvector;
--  ?column?
-- ----------
--  f
```

Also supported `@@` combinations:

- `tsvector @@ tsquery`
- `tsquery @@ tsvector`
- `text @@ tsquery`, equivalent to `to_tsvector(x) @@ tsquery`
- `text @@ text`, equivalent to `to_tsvector(x) @@ plainto_tsquery(y)`


### tsquery

A `tsquery` contains search terms, which must be already-normalized lexemes, and may combine multiple terms using special operators. There are functions like `to_tsquery`, `plainto_tsquery`, and `phraseto_tsquery` that are helpful in converting user-provided text into a proper `tsquery`, primarily by normalizing words appearing in the text.

Special operators:

- `AND (&)`

    Both words should be present on the document

- `OR (|)`

    At least one word should be present on the document

- `NOT (!)`

    The word should not be present on the document

- `FOLLOWED BY (<N>, <->)`

    `N` being the exact number of lexemes one should be apart from another. The `<->` operator is equivalent to `<1>`.

- `Parenthesis ()`

    Used to nest and specify priority of operators


## Tables and indexing

Although it is possible to do a full text search without an index, it is highly recommended if it aims to be a common occurrence.

### Crating indexes

We use a `GIN` index to speed up text searches.

```sql
--                                   configuration name ----
--                                                         |
--                                                         v
CREATE INDEX pgweb_idx ON pgweb USING GIN (to_tsvector('english', body));
```

Only text search functions that specify a configuration name can be used in expression indexes. This is because the index contents must be unaffected by the `default_text_search_config`. If they were affected, the index contents might be inconsistent because different entries could contain `tsvector`s created with different configurations. Accordingly, only 

It is also possible to set up more complex expression indexes wherein the configuration name is specified by another column. This allows mixed configurations in the same index while recording which configuration was used for each index entry. Useful for documents in different languages.

```sql
--        column name containing the configuration name ----
--                                                         |
--                                                         v
CREATE INDEX pgweb_idx ON pgweb USING GIN (to_tsvector(config_name, body));
```

Indexes can even concatenate columns:

```sql
CREATE INDEX pgweb_idx ON pgweb USING GIN (to_tsvector('english', title || ' ' || body));
```

Additionally, a special `tsvector` column can be created to hold the `to_tsvector` result. To keep this column automatically up to date with the its source data, use a stored generated column and later create a `GIN` index for it.


```sql
ALTER TABLE pgweb
    ADD COLUMN textsearchable_index_col tsvector
        GENERATED ALWAYS AS (
            to_tsvector('english', coalesce(title, '') || ' ' || coalesce(body, ''))
        ) STORED;

CREATE INDEX textsearch_idx ON pgweb USING GIN (textsearchable_index_col);
```

The advantage of the later approach is to never having to specify the text search configuration in queries in order to make use of the index. Additionally, searches will be faster, since it will not be necessary to redo the `to_tsvector` calls to verify index matches.

## Methods

### to_tsvector

```
to_tsvector([ config regconfig, ] document text) returns tsvector
```

It parses a textual document into tokens, reduces the token to lexemes, and returns a `tsvector` which lists the lexemes together with their positions in the document.

```sql
SELECT to_tsvector('english', 'a fat cat sat on a mat - it ate a fat rats');
--                     to_tsvector
-- -----------------------------------------------------
--  'ate':9 'cat':3 'fat':2,11 'mat':7 'rat':12 'sat':4
```

Internally, the `to_tsvector` calls a `parser` which breaks the document text into tokens and assigns a type to each token. For each token, a list of dictionaries is consulted, where the list can vary depending on the token type. The first dictionary that recognizes the token emits one or more normalized lexemes to represent the token. Some words are recognized as `stop words`, which causes them to be ignored since the occur too frequently to be useful in searching. If no dictionary in the list recognizes the token, then it is also ignored. The choice of parser, dictionaries and which types of tokens to index are determined by the selected text search configuration.

### to_tsquery

```
to_tsquery([ config regconfig, ] querytext text) returns tsquery
```

It creates a `tsquery` value from `querytext`, which must consist of single tokens separated by the special operators (`&`, `|`, `!`, `<->`, `<N>`), possibly grouped using parenthesis. In other words, the input to `to_tsquery` must already follow the general rules for `tsquery` input.

```sql
SELECT to_tsquery('english', 'The & Fat & Rats');
--   to_tsquery
-- ---------------
--  'fat' & 'rat'
```

Weights can also be attached to each lexeme to restrict it to match only `tsvector` lexemes of those weights.

```sql
SELECT to_tsquery('english', 'The | Rats:AB');
--   to_tsquery
-- ---------------
--  'fat' & 'rat':AB
```

Also, `*` can be attached to a lexeme to specify prefix matching, effectively making it match against any word in a `tsvector` that begins with the given string:

```sql
SELECT to_tsquery('supern:*A & star:A*B');
--         to_tsquery
-- --------------------------
--  'supern':*A & 'star':*AB
```

Single-quoted phrases are also admitted, which is primarily useful when the configuration includes a `thesaurus` dictionary that may trigger on such phrases.

```sql
SELECT to_tsquery('''supernovae starts'' & !crab');
--         to_tsquery
-- --------------------------
--  'sn' & ! 'crab'
```

### plainto_tsquery

```
plainto_tsquery([ config regconfig, ] querytext text) returns tsquery
```

It transforms the unformatted text `querytext` to a `tsquery` value. The text is parsed and normalized much as for `to_tsvector`, then the `&` operator is inserted between surviving words.

```sql
SELECT plainto_tsquery('english', 'The Fat Rats');
--  plainto_tsquery
-- -----------------
--  'fat' & 'rat'
```

It will also not recognize `tsquery` operators, but will discard them as if they were plain characters that must be discarded.

```sql
SELECT plainto_tsquery('english', 'The Fat & Rats:C');
--    plainto_tsquery
-- ---------------------
--  'fat' & 'rat' & 'c'
```

### phraseto_tsquery

```
phraseto_tsquery([ config regconfig, ] querytext text) returns tsquery
```

It behaves much like `plainto_tsquery`, except that it inserts `<->` operators between surviving words instead of the `&` operator. Also, stop words are not simply discarded, but are accounted for by inserting `<N>` operators rather than `<->`. This is useful when searching for exact lexemes sequences.

```sql
SELECT phraseto_tsquery('english', 'The Fat Rats');
--  plainto_tsquery
-- -----------------
--  'fat' <-> 'rat'
```

Like `plainto_tsquery`, it will not recognize `tsquery` operators.


### websearch_to_tsquery

```
websearch_to_tsquery([ config regconfig, ] querytext text) returns tsquery
```

It creates a `tsquery` value from `querytext` using an alternative syntax in which simple unformatted text is a valid query. It does recognizes certain operators, although not special ones. This function never raises syntax errors, which makes it possible to use raw user-supplied input for search. The following syntax is supported:

- unquoted text

    Text not inside quote marks will be converted to terms separated by & operators, as if processed by `plainto_tsquery`
- "quoted text"

    Text inside quote marks will be converted to terms separated by <-> operators, as if processed by `phraseto_tsquery`
- OR

    The "or" word will be converted to the `|` operator
- \-

    A dash will be converted to the `!` operator

```sql
SELECT websearch_to_tsquery('english', 'The fat rats');
--  websearch_to_tsquery
-- ----------------------
-- 'fat' & 'rat'

SELECT websearch_to_tsquery('english', '"supernovae stars" -crab');
--        websearch_to_tsquery
-- --------------------------------
-- 'supernva' <-> 'star' & ! 'crab'

SELECT websearch_to_tsquery('english', '"sad cat" or "fat rat"');
--       websearch_to_tsquery
-- ---------------------------------
-- 'sad' <-> 'cat' | 'fat' <-> 'rat'

SELECT websearch_to_tsquery('english', 'signal -"segmentation fault"');
--         websearch_to_tsquery
-- -------------------------------------
-- 'signal' & !( 'segment' <-> 'fault' )
```

### ts_headline

```
ts_headline([ config regconfig ] document text, query tsquery [, options text]) returns text
```

It accepts a document along with a query, and returns an excerpt from the document in which terms from the query are highlighted.

#### Options

If an `options` string is specified it must consist of a comma-separated list of one or more `option=value` pairs:

- `MaxWords`, `MinWords`: `integer`

    These numbers determine the longest and the shortest headlines to output. Defaults are 35 and 15.
- `ShortWord`: `integer`

    Words of this length or less will be dropped at the start and end of a headline, unless they are query terms. Default value is 3.
- `HighlightAll`: `boolean`

    If `true`, the whole document will be used as the headline, ignoring the preceding three parameters. Default is `false`.
- `MaxFragments`: `integer`

    Maximum number of text fragments to display. The default is zero, which selects a `non-fragmented-based headline generation model`. A value greater than zero selects `fragmented-based headline generation`.
- `StartSel`, `StopSel`: `string`

    The strings with which to delimit query words appearing in the document, to distinguish them from other excerpted words. Default values are `<b>` and `<\b>`.
- `FragmentDelimiter`: `string`

    When more than one fragment is displayed, the fragments will be separated by this string. Default value is `...`.

These option names are recognized case-insensitively. You must double-quote string values if the contain spaces or commas.


### querytree

```
querytree(query tsquery) returns text
```

Returns the portion of a `tsquery` that can be used for searching an index. This function is useful for detecting unindexable queries.

```sql
SELECT querytree(to_tsquery('defined'));
--  querytree
-- ----------
-- 'defin'

SELECT querytree(to_tsquery('!defined'));
--  querytree
-- ----------
-- T
```

### ts_rewrite

The `ts_rewrite` family of functions search a given `tsquery` for occurrences of a `target` subquery, and replace each occurrence with a `substitute` subquery. In essence this operation is a `tsquery`-specific version of substring replacement.

There is an overlap in functionality between this feature and thesaurus dictionaries, however, you can modify a set of rewrite rules on-the-fly without reindexing, whereas updating the thesaurus requires reindexing to be effective.

```
ts_rewrite(query tsquery, target tsquery, substitute tsquery) returns tsquery
```

This form of `ts_rewrite` simply applies a single rewrite rule: `target` is replaced by `substitute` wherever it appears in `query`.

```sql
SELECT ts_rewrite('a & b'::tsquery, 'a'::tsquery, 'c'::tsquery);
--  ts_rewrite
-- ------------
-- 'b' & 'c'
```

```
ts_rewrite(query tsquery, select text) returns tsquery
```

This form of `ts_rewrite` accepts a starting `query` and a `SELECT` command, which is given as a text string. The `SELECT` must yield two columns of `tsquery` type. For each row of `SELECT` result, occurrences of the first column value (`target`) are replaced by the second column value (`substitute`) within the current `query` value.

```sql
CREATE TABLE aliases (t tsquery PRIMARY KEY, s tsquery);
INSERT INTO aliases VALUES('a', 'c');

SELECT ts_rewrite('a & b'::tsquery, 'SELECT t,s FROM aliases');
--  ts_rewrite
-- ------------
-- 'b' & 'c'
```

> Rewriting can be slow when there are many rewriting rules, since it checks every rule for a possible match. To filter the obvious non-candidate rules we can use the containment operator for the `tsquery` type.

```sql
SELECT ts_rewrite(
    'a & b'::tsquery,
    'SELECT t,s FROM aliases WHERE ''a & b''::tsquery @> t'
);
```


## Ranking search results

Ranking attempts to measure how relevant documents are to a particular query, so that when there are many matches the most relevant ones can be shown first. PostgreSQL provides two predefined ranking functions, which take into account lexical proximity and structural information; that is, they consider how often the query terms appear in the document, how close together are the terms in the document, and how important is the part of the document where the occur. There is also the possibility to provide your own ranking functions to fit your specific needs.

### ts_rank

```
ts_rank([ weights float4[], ] vector tsvector, query tsquery [, normalization integer]) returns float4
```

Ranks vectors based on the frequency of their matching lexemes.

### ts_rank_cd

```
ts_rank_cd([ weights float4[], ] vector tsvector, query tsquery [, normalization integer]) returns float4
```

It computes the `cover density` ranking for the given document vector and query, as described in [Clarke, Cormack and Tudhope's "Relevance ranking for one to three term queries"](https://dl.acm.org/doi/abs/10.5555/2856695.2856730). This function requires lexeme positional information to perform its calculations. Therefore, it ignores "stripped" lexemes in the `tsvector`. If there are no unstripped lexemes in the input, the result will be zero.

### Weights

For both ranking functions, the optional `weights` argument offers the ability to weigh word instances more or less heavily depending on how the are labeled. The weight arras specify how heavily to weigh each category of words.

```
# Order
{D-weight, C-weight, B-weight, A-weight}

# Defaults
{0.1, 0.2, 0.4, 1.0}
```

### Normalization

Since longer documents have a greater chance of containing a query term, it is reasonable to take into account document size. Both ranking functions take an integer `normalization` option that specifies whether and how a document's length should impact is rank. The integer option controls several behaviors, so its a bit mask:

- `0`

    The default. It ignores the document length.
- `2`

    Divides the rank by 1 + the logarithm of the document length.
- `4`

    Divides the rank by the document length.
- `8`

    Divides the rank by the mean harmonic distance between extents. Only implemented by `ts_rank_cd`.
- `16`

    Divides the rank by the number of unique words in the document.
- `32`

    Divides the rank by itself + 1. Useful for scaling all ranks into a 0 to 1 range.

> Ranking can be expensive since it requires consulting the `tsvector` of each matching document, which can be `I/O` bound and therefore slow. Unfortunately, it is almost impossible to avoid since practical queries often result in large numbers of matches.


## Limitations

- The length of each lexeme must be less than 2 kilobytes.
- The length of a `tsvector` (lexemes + positions) must be less than 1 megabyte.
- The number of lexemes must be less than 2^64
- Position values in `tsvector` must be grater than 0 and no more than 16,383
- The match distance in a `<N>` `tsquery` operator cannot be more than 16,384
- No more than 256 positions per lexeme
- The number of nodes (lexemes + operators) in a `tsquery` must be less than 32,768

## Notes

- If a searchable columns is able to be `NULL`, the `coalesce(column, '')` mechanism should by used to prevent `NULL` results for the whole document.


## Resources

- [Postgres Full-Text Search: A Search Engine in a Database](https://www.crunchydata.com/blog/postgres-full-text-search-a-search-engine-in-a-database)
- [Chapter 12. Full Text Search](https://www.postgresql.org/docs/15/textsearch.html)
